{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0isW8Vnfl4h5"
      },
      "outputs": [],
      "source": [
        "#Importing Modules\n",
        "import re, string, unicodedata\n",
        "import nltk\n",
        "import inflect\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "from gensim.summarization.summarizer import summarize    # 3.8.3\n",
        "\n",
        "import glob\n",
        "import textract\n",
        "import warnings\n",
        "import PyPDF2\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
        "\n",
        "\n",
        "# Making a class for the final output object\n",
        "class ResultElement:\n",
        "    def __init__(self, rank, filename):\n",
        "        self.rank = rank\n",
        "        self.filename = filename\n",
        "# Function to give file location\n",
        "def getfilepath(loc):\n",
        "    temp = str(loc)\n",
        "    temp = temp.replace('\\\\', '/')\n",
        "    return temp\n",
        "\n",
        "# Function to remove non ascii words from text\n",
        "def remove_non_ascii(words):\n",
        "    '''to remove all non-ASCII char from tokenized words'''\n",
        "    words_res = []\n",
        "    for word in words:\n",
        "        word_new = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        words_res.append(word_new)\n",
        "    return words_res\n",
        "\n",
        "# Function to convert text from uppercase to lowercase\n",
        "def to_lowercase(words):\n",
        "    '''converting to lowercase '''\n",
        "    words_res = []\n",
        "    for word in words:\n",
        "        word_new = word.lower()\n",
        "        words_res.append(word_new)\n",
        "    return words_res\n",
        "\n",
        "# Function to remove punctuations\n",
        "def remove_punctuation(words):\n",
        "    ''' removing punctuations'''\n",
        "    words_res = []\n",
        "    for word in words:\n",
        "        word_new = re.sub(r'[^\\w\\s]', '', word)\n",
        "        if word_new != '':\n",
        "            words_res.append(word_new)\n",
        "    return words_res\n",
        "\n",
        "# Function to replace the number with their English spellings\n",
        "def replace_numbers(words):\n",
        "    '''replacing integer in list of tokenized words to text form'''\n",
        "    p = inflect.engine()\n",
        "    words_res = []\n",
        "    for word in words:\n",
        "        if word.isdigit():\n",
        "            word_new = p.number_to_words(word)\n",
        "            words_res.append(word_new)\n",
        "        else:\n",
        "            words_res.append(word)\n",
        "    return words_res\n",
        "\n",
        "\n",
        "# Function to remove stopwords from the texts\n",
        "def remove_stopwords(words):\n",
        "    '''removing stopwords from tokenized word list'''\n",
        "    words_res = []\n",
        "    for word in words:\n",
        "        if word not in stopwords.words('english'):\n",
        "            words_res.append(word)\n",
        "    return words_res\n",
        "\n",
        "# Function to stem tokenized words\n",
        "def stem_words(words):\n",
        "    '''stemming words in tokenized words list'''\n",
        "    stemmer = LancasterStemmer()\n",
        "    stems = []\n",
        "    for word in words:\n",
        "        stem = stemmer.stem(word)\n",
        "        stems.append(stem)\n",
        "    return stems\n",
        "\n",
        "# Function to lemmatize tokenized words\n",
        "def lemmatize_verbs(words):\n",
        "    '''lemmatizing verbs in tokenized words list'''\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []\n",
        "    for word in words:\n",
        "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "        lemmas.append(lemma)\n",
        "    return lemmas\n",
        "\n",
        "\n",
        "# A Normalization function used to run all the above functions on the text at once\n",
        "def normalize(words):\n",
        "    words = remove_non_ascii(words)\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_punctuation(words)\n",
        "    words = replace_numbers(words)\n",
        "    words = remove_stopwords(words)\n",
        "    words = stem_words(words)\n",
        "    words = lemmatize_verbs(words)\n",
        "    return words\n",
        "\n",
        "\n",
        "# lists declaration\n",
        "Resume_Vector = []\n",
        "Ordered_list_Resume = []\n",
        "Ordered_list_Resume_Score = []\n",
        "LIST_OF_FILES = []\n",
        "\n",
        "\n",
        "#Separate lists for differentiating between pdf , doc, and docx file\n",
        "LIST_OF_FILES_PDF = []\n",
        "LIST_OF_FILES_DOC = []\n",
        "LIST_OF_FILES_DOCX = []\n",
        "\n",
        "Resumes_File_Names = []\n",
        "Resumes = []\n",
        "\n",
        "\n",
        "Temp_pdf = ''\n",
        "\n",
        "# changing directory to folder where our resumes are stored\n",
        "\n",
        "os.chdir(r'C:\\Users\\VISHWAS\\Desktop\\Wabatech Project\\Project 2\\My Progress')\n",
        "\n",
        "# here in single quotes put the address of the folder where you have resumes in pdf, doc or docx format\n",
        "\n",
        "\n",
        "#searching for pdf, doc and docs and separating them\n",
        "for file in glob.glob('**/*.pdf', recursive=True):\n",
        "    LIST_OF_FILES_PDF.append(file)\n",
        "for file in glob.glob('**/*.doc', recursive=True):\n",
        "    LIST_OF_FILES_DOC.append(file)\n",
        "for file in glob.glob('**/*.docx', recursive=True):\n",
        "    LIST_OF_FILES_DOCX.append(file)\n",
        "\n",
        "\n",
        "#making one list out of pdf, doc and docx\n",
        "LIST_OF_FILES = LIST_OF_FILES_DOC + LIST_OF_FILES_DOCX + LIST_OF_FILES_PDF\n",
        "\n",
        "#checking all Resumes files name\n",
        "print(\"\\n\")\n",
        "print(\"-------------This is LIST OF FILES--------------------\")\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "for i,j in enumerate(LIST_OF_FILES):\n",
        "    print(str(i)+\"----\"+str(j))\t #printing each resume name with serial no\n",
        "    print('\\n')\n",
        "\n",
        "\n",
        "\n",
        "#for going through each resume\n",
        "for no,i in enumerate(LIST_OF_FILES):\n",
        "    Ordered_list_Resume.append(i)\n",
        "    Temp = i.split(\".\")\n",
        "#if the resume is in pdf format, extracting the text\n",
        "    if Temp[1] == \"pdf\" or Temp[1] == \"Pdf\" or Temp[1] == \"PDF\":\n",
        "        try:\n",
        "            print('----------------------------------------------------------------------------------')\n",
        "            print(\"\\n\\nThis is PDF\\n\\n\"+str(no)+\"    \"+str(i))\n",
        "            with open(i,'rb') as pdf_file:\n",
        "                read_pdf = PyPDF2.PdfFileReader(pdf_file,strict=False)\n",
        "                # page = read_pdf.getPage(0)\n",
        "                # page_content = page.extractText()\n",
        "                # Resumes.append(Temp_pdf)\n",
        "\n",
        "                number_of_pages = read_pdf.getNumPages()\n",
        "                for page_number in range(number_of_pages):\n",
        "\n",
        "                    page = read_pdf.getPage(page_number)\n",
        "                    page_content = page.extractText()\n",
        "                    page_content = page_content.replace('\\n', ' ')\n",
        "                    # page_content.replace(\"\\r\", \"\")\n",
        "                    Temp_pdf = str(Temp_pdf) + str(page_content)\n",
        "                    # Temp_pdf.append(page_content)\n",
        "                    # print(Temp_pdf)\n",
        "\n",
        "\n",
        "\n",
        "                '''\n",
        "                Temp_pdf = nltk.word_tokenize(Temp_pdf)\n",
        "                Temp_pdf = normalize(Temp_pdf)\n",
        "                Temp_pdf = ' '.join(map(str, Temp_pdf))\n",
        "                '''\n",
        "\n",
        "                #print(Temp_pdf)\n",
        "\n",
        "\n",
        "                #print('----------------------------------------------------------------------------------')\n",
        "\n",
        "\n",
        "\n",
        "                Resumes.extend([Temp_pdf])\n",
        "                Temp_pdf = ''\n",
        "                # f = open(str(i)+str(\"+\") , 'w')\n",
        "                # f.write(page_content)\n",
        "                # f.close()\n",
        "\n",
        "\n",
        "        except Exception as e: print(e)\n",
        "\n",
        "#if the resume is in doc, extracting text from it\n",
        "    if Temp[1] == \"doc\" or Temp[1] == \"Doc\" or Temp[1] == \"DOC\":\n",
        "        print(\"This is DOC\\n\" , i)\n",
        "\n",
        "        try:\n",
        "            a = textract.process(i)\n",
        "            a = a.replace(b'\\n',  b' ')\n",
        "            a = a.replace(b'\\r',  b' ')\n",
        "            a = a.replace(b'\\t',  b' ')\n",
        "            b = str(a)\n",
        "\n",
        "            '''\n",
        "            b = nltk.word_tokenize(b)\n",
        "            b = normalize(b)\n",
        "            b = ' '.join(map(str, b))\n",
        "            '''\n",
        "            c = [b]\n",
        "\n",
        "\n",
        "            #print(c)\n",
        "            #print(\"\\n\\n\\n\\n\\n\")\n",
        "\n",
        "\n",
        "            Resumes.extend(c)\n",
        "        except Exception as e: print(e)\n",
        "\n",
        "  #if the resume is in docx, extracting text from it\n",
        "\n",
        "    if Temp[1] == \"docx\" or Temp[1] == \"Docx\" or Temp[1] == \"DOCX\":\n",
        "        print(\"This is DOCX\\n\" , i)\n",
        "        try:\n",
        "            a = textract.process(i)\n",
        "            a = a.replace(b'\\n',  b' ')\n",
        "            a = a.replace(b'\\r',  b' ')\n",
        "            a = a.replace(b'\\t',  b' ')\n",
        "\n",
        "            b = str(a)\n",
        "\n",
        "            '''\n",
        "            b = nltk.word_tokenize(b)\n",
        "            b = normalize(b)\n",
        "            b = ' '.join(map(str, b))\n",
        "            '''\n",
        "\n",
        "            c = [b]\n",
        "\n",
        "\n",
        "            #print(c)\n",
        "            #print(\"\\n\\n\\n\\n\\n\")\n",
        "\n",
        "\n",
        "            Resumes.extend(c)\n",
        "        except Exception as e: print(e)\n",
        "\n",
        "\n",
        "    if Temp[1] == \"ex\" or Temp[1] == \"Exe\" or Temp[1] == \"EXE\":\n",
        "        print(\"This is EXE\" , i)\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "print(\"Done Parsing.\")\n",
        "\n",
        "#just cheking the correct working directory , can remove\n",
        "print(os.getcwd())\n",
        "\n",
        "'''\n",
        "print(\"this is resumes list ----------\")\n",
        "\n",
        "count=0\n",
        "for i in Resumes:\n",
        "\n",
        "    print(\"--------------------------------------\")\n",
        "    print(i)\n",
        "    count+=1\n",
        "    print('\\n\\n')\n",
        "print(count)\n",
        "'''\n",
        "\n",
        "\n",
        "# now job description part\n",
        "\n",
        "Job_Desc = 0\n",
        "LIST_OF_TXT_FILES = []\n",
        "\n",
        "\n",
        "\n",
        "f = open(\"oaktree.txt\", 'r')\n",
        "\n",
        "#here put the address of the job description file(in my case it was the same folder where my code was stored, that’s why I directly given the name)\n",
        "\n",
        "text = f.read()\n",
        "\n",
        "f.close()\n",
        "\n",
        "\n",
        "#extracting text from JD and summarizing it\n",
        "try:\n",
        "    t = str(text)\n",
        "    t = summarize(t, word_count=200)\n",
        "\n",
        "    t = nltk.word_tokenize(t)\n",
        "    t = normalize(t)\n",
        "    t = ' '.join(map(str, t))\n",
        "\n",
        "    text = [t]\n",
        "\n",
        "except:\n",
        "    text = 'None'\n",
        "\n",
        "\n",
        "print(text)\n",
        "print(\"\\n\\n\\n\\n\\n\")\n",
        "\n",
        "\n",
        "\n",
        "#final model, applying KNN\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "# print(text)\n",
        "vectorizer.fit(text)\n",
        "vector = vectorizer.transform(text)\n",
        "\n",
        "Job_Desc = vector.toarray()\n",
        "\n",
        "\n",
        "print(text)\n",
        "print(\"\\n\\n\\n\\n\\n\")\n",
        "\n",
        "count=0\n",
        "for i in Resumes:\n",
        "    count+=1\n",
        "    text = i\n",
        "    t = str(text)\n",
        "    try:\n",
        "        t = summarize(t)\n",
        "\n",
        "        t = nltk.word_tokenize(t)\n",
        "        t = normalize(t)\n",
        "        t = ' '.join(map(str, t))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        text = [t]\n",
        "        vector = vectorizer.transform(text)\n",
        "\n",
        "        aaa = vector.toarray()\n",
        "        '''\n",
        "\n",
        "        print(\"\\n\\n\\n\\n\\n\")\n",
        "        print(aaa)\n",
        "\n",
        "        print(\"----------------------------------------------------\")\n",
        "        print(aaa)\n",
        "        print(\"----------------------------------------------------\")\n",
        "        print('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')\n",
        "        '''\n",
        "\n",
        "        Resume_Vector.append(vector.toarray())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"resume no ---\"+str(count))\n",
        "        print(e)\n",
        "\n",
        "\n",
        "for i in Resume_Vector:\n",
        "\n",
        "    samples = i\n",
        "    neigh = NearestNeighbors(n_neighbors=1)\n",
        "    neigh.fit(samples)\n",
        "    NearestNeighbors(algorithm='auto', leaf_size=30)\n",
        "\n",
        "    Ordered_list_Resume_Score.extend(neigh.kneighbors(Job_Desc)[0][0].tolist())\n",
        "\n",
        "\n",
        "Z = [x for _,x in sorted(zip(Ordered_list_Resume_Score,Ordered_list_Resume))]\n",
        "\n",
        "print(Ordered_list_Resume)\n",
        "print(Ordered_list_Resume_Score)\n",
        "\n",
        "final_res = []\n",
        "\n",
        "\n",
        "#printing the result, using the already defined class for result\n",
        "for n,i in enumerate(Z):\n",
        "    # print(\"Rank\\t\" , n+1, \":\\t\" , i)\n",
        "    # final_res.append(str(\"Rank\\t\" , n+1, \":\\t\" , i))\n",
        "    name = getfilepath(i)\n",
        "    #name = name.split('.')[0]\n",
        "    rank = n+1\n",
        "    res = ResultElement(rank, name)\n",
        "    final_res.append(res)\n",
        "    # res.printresult()\n",
        "    print(f\"Rank{res.rank+1} :\\t {res.filename}\")\n"
      ]
    }
  ]
}